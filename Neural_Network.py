# -*- coding: utf-8 -*-
"""Progetto_ML_Gentile_&_Belli_Contarini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jHnFWz8L_bV1ypzrIV9UhxOm39NWm_i-

#Outline del progetto: *ML applications in NMR (Osteoporosis)*
*Autori:* Andrea Gentile & Andrea Belli Contarini

###[Link](https://drive.google.com/drive/folders/1zDFIiy2POWa2ve0Lu6XFwuKReKFK43H_?usp=drive_link) al google folder contenente codice, risultati, presentazione, dati e mini relazione (3 pagine)

### Definizione degli Obiettivi
Alleanare un classificatore che sia in grado di distinguire pazienti sani da pazienti osteoporotici a partire da una serie di immagini prese in MRI, riducendole ad uno spazio latente su cui operare la classificazione.

### 1. Preprocessamento dei Dati
- **Database**:
Il database delle immagini MRI sull’osteoporosi è composto da 49 soggetti. Per tutti i soggetti sono state acquisite tra le 6 e le 9 immagini a diverse profondità del piede, pesate in $T_2$ (9 immagini a diverso tempo di echo, $T_E$), mentre solo per 35 di loro sono state acquisite anche quelle pesate in $T_1$ ( 10 a diverso tempo di ripetizione, $T_R$). Il database finale consta di 12 soggetti sani con immagini pesate in $T_1$ e $T_2$, e 21 soggetti osteoporotici. I soggetti esclusi sono classificati come osteopenici oppure non avevano entrambe le acquisizioni.
Per le pesate in $T_2$, sono state acquisite le immagini a 9 diversi $T_E$ = (25, 50, 75, 100, 125, 150, 175, 200, 225) $ms$ su diverse fette, mentre quelle in $T_1$ sono state acquisite con 10 diversi $T_R$ = (280, 320, 360, 400, 500, 600, 800, 1000, 1200, 2000) $ms$ su un’unica fetta centrale.

- **Preprocessamento**:
  - Passaggio da .nii ad .npy
  - Successiva **normalizzazione** delle immagini durante la costruzione del *DataLoader*: dividiamo ogni *slice* $T_2$ per la prima fetta (che ha $T_E = 25 ms$). Questo passaggio serve per eliminare le differenze di intensità assoluta tra le immagini, rendendo i dati NMR più coerenti e comparabili.

*Dettagli Tecnici:* La prima fetta ($T_E = 25 ms$) è utilizzata come riferimento per la normalizzazione. Ogni canale dell'immagine $T_2$ viene diviso per il corrispondente valore della prima fetta. L'operazione `torch.clamp(t2_tensor[:, 0:1,:,:], min=epsilon)` assicura che i valori della prima fetta non siano zero, evita così le divisioni per zero.
Rimozione della Prima Fetta: Dopo la normalizzazione, la prima fetta viene rimossa, lasciando un tensore con 8 slice per ciascun canale $T_2$: `(6, 8, 512, 512)`.


### 2. Modellazione
Ultilizzare un classificatore che:
- esegua una convoluzione per ogni singola fetta sul logaritmo dei segnali a tutti i tempi, per ricavare l'informazione del decadimento in $T_1$ o $T_2$;
- eseguire convoluzioni separate su ogni fetta tramite l'attributo "groups";
- accorpare l'informazione di tutte le fette e relativi canali;
- passare ad uno spazio latente su cui eseguire una classificazione.

### 3. Feature Selection
  - **Valutazione**: Dividere il dataset in training e testing set, utilizzare **Jackknife Cross-Validation** per ottenere *accuracy, precision, recall*, e *F1-score*.
  - **Feature Selection**: Indipendent Component Analysis.

## Librerie e mount Google Drive
"""

#Caricamento Librerie
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
import os
import cv2
from math import e
import matplotlib.colors as mcolors

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.utils import make_grid
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import random
from torchsummary import summary
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.metrics import roc_curve, auc


import warnings
from sklearn.decomposition import PCA
from sklearn.decomposition import FastICA
from sklearn.manifold import TSNE
!pip install umap-learn
import umap

#Connettiamo a Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Creamo i percorsi per raggiungere i dati nel drive
health_path = "/content/drive/MyDrive/osteoporosi/Healthy"
osteoporosis_path = "/content/drive/MyDrive/osteoporosi/Osteoporosis"

#Parametro per regolare l'esecuzione di alcune celle
run_cell = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("torch.device in utilizzo =", device)

"""##1) Preprocessamento delle immagini

"""

from google.colab import drive
drive.mount('/content/drive')

if False: # Change to True if you want to run the cell
    print("Cell is running")

    #Funzione per caricamento immaggini niftii
    def load_nii(file_path):
        img = nib.load(file_path)
        img_data = img.get_fdata()
        return img_data

    # Funzione per preprocessare i dati
    def preprocess_data(data_path):
        patients = os.listdir(data_path)
        for patient in patients:
            patient_path = os.path.join(data_path, patient, 'dwi')
            t1_path = os.path.join(patient_path, 'fse_t1.nii')
            t2_path = os.path.join(patient_path, 'fse_t2.nii')

            if os.path.exists(t1_path) and os.path.exists(t2_path):
                t1_data = load_nii(t1_path)
                t2_data = load_nii(t2_path)

                # Salva i dati preprocessati per utilizzo successivo
                np.save(os.path.join(patient_path, 't1_preprocessed.npy'), t1_data)
                np.save(os.path.join(patient_path, 't2_preprocessed.npy'), t2_data)


                # Print degli shapes delle loaded images
                print(f"{patient} - T1 shape: {t1_data.shape}, T2 shape: {t2_data.shape}")
else:
  print("Cell is skipped")

if False: # Change to True if you want to run the cell
    print("Cell is running")

    # Preprocessamento dei dati dei pazienti sani e osteoporotici - senza ridimensionamento
    preprocess_data(health_path)
    preprocess_data(osteoporosis_path)
else:
    print("Cell is skipped")

"""Immagini con **shape**: **(A, B, C, D)**, con:


*   **A** = Numero di ***slices*** lungo l'asse z del campione
*   **B** & **C** = Dimensione spaziale (2D) dell'immagine (#pixel)
*   **D** = Numero dei diversi tempi di acquisizione ($T_E$ per $T_2$ e $T_R$ per $T_1$).

Immagini pesate in $T_1$: `(1,512,512,10)`
Immagini pesate in $T_2$: `(6÷9,512,512,9)`

### Visualizzazione Immagini
"""

# Funzione per caricare le immagini preprocessate e selezionare una slice casuale
def load_random_slices(data_path, condition, num_examples=4):
    patients = os.listdir(data_path)
    example_images = []
    for patient in random.sample(patients, num_examples):
        patient_path = os.path.join(data_path, patient, 'dwi')
        t1_preprocessed_path = os.path.join(patient_path, 't1_preprocessed.npy')
        t2_preprocessed_path = os.path.join(patient_path, 't2_preprocessed.npy')

        if os.path.exists(t1_preprocessed_path) and os.path.exists(t2_preprocessed_path):
            t1_data = np.load(t1_preprocessed_path)
            t2_data = np.load(t2_preprocessed_path)

            # Verifica delle dimensioni delle immagini
            if len(t1_data.shape) != 4 or len(t2_data.shape) != 4:
                print(f"Warning: Dimensioni non valide per il paziente {patient}. Skipping.")
                continue

            # Estrarre una fetta casuale per visualizzazione
            t1_slice_index = random.randint(0, t1_data.shape[-1] - 1)
            t2_slice_index = random.randint(0, t2_data.shape[-1] - 1)
            t1_slice = t1_data[0, :, :, t1_slice_index].squeeze()
            t2_slice = t2_data[0, :, :, t2_slice_index].squeeze()

            example_images.append((t1_slice, t2_slice, patient, condition))
    return example_images

# Funzione per visualizzare le immagini
def show_random_slices(images, title):
    fig, axes = plt.subplots(len(images), 2, figsize=(10, len(images) * 5))

    for i, (t1_img, t2_img, patient, condition) in enumerate(images):
        axes[i, 0].imshow(t1_img, cmap='gray')
        axes[i, 0].set_title(f"{patient} - {condition} - T1 Slice")
        axes[i, 0].axis('off')

        axes[i, 1].imshow(t2_img, cmap='gray')
        axes[i, 1].set_title(f"{patient} - {condition} - T2 Slice")
        axes[i, 1].axis('off')

    plt.tight_layout()
    plt.suptitle(title)
    plt.subplots_adjust(top=0.95)
    plt.show()

# Caricare e visualizzare 4 immagini a titolo di esempio
example_images_health = load_random_slices(health_path, 'Healthy')
example_images_osteoporosis = load_random_slices(osteoporosis_path, 'Osteoporosis')
if False:
  # Visualizzare le immagini di esempio
  show_random_slices(example_images_health + example_images_osteoporosis, "Random Slices from Preprocessed Images")
else:
  print("No printing of images")

# Funzione per visualizzare tutte le immagini T1 o T2 per un paziente
def plot_all_temporal_slices(image_data, title):
    num_slices = image_data.shape[-1]  # Numero di fette temporali
    fig, axes = plt.subplots(1, num_slices, figsize=(num_slices * 6, 6))

    for i in range(num_slices):
        axes[i].imshow(image_data[0, :, :, i], cmap='gray')
        axes[i].set_title(f'Slice at {i+1} Time')
        axes[i].axis('off')

    plt.suptitle(title)
    plt.show()

# Funzione per visualizzare tutte le fette di una singola acquisizione T2
def plot_all_spatial_slices(t2_data, title):
    num_slices = t2_data.shape[0]  # Numero di fette spaziali
    fig, axes = plt.subplots(1, num_slices, figsize=(num_slices * 6, 6))

    for i in range(num_slices):
        axes[i].imshow(t2_data[i, :, :, 0], cmap='gray')  # Consideriamo solo la prima acquisizione temporale
        axes[i].set_title(f'Slice {i+1} of Foot')
        axes[i].axis('off')

    plt.suptitle(title)
    plt.show()

if False:
  # Selezionare un paziente random
  patients = os.listdir(health_path)
  random_patient = random.choice(patients)
  patient_path = os.path.join(health_path, random_patient, 'dwi')

  # Caricare i dati T1 e T2 per il paziente selezionato
  t1_preprocessed_path = os.path.join(patient_path, 't1_preprocessed.npy')
  t2_preprocessed_path = os.path.join(patient_path, 't2_preprocessed.npy')

  t1_data = np.load(t1_preprocessed_path)
  t2_data = np.load(t2_preprocessed_path)

  # Visualizzare tutte le immagini temporali T1
  plot_all_temporal_slices(t1_data, f'T1 Temporal Slices for Patient {random_patient}')

  # Visualizzare tutte le immagini temporali T2
  plot_all_temporal_slices(t2_data, f'T2 Temporal Slices for Patient {random_patient}')

  # Visualizzare tutte le fette di un singolo T2
  plot_all_spatial_slices(t2_data, f'All Spatial Slices for Single T2 of Patient {random_patient}')
else:
  print("Cell is skipped")

"""##2) Modellazione

### Dataloader
"""

def group_preprocessed_data(data_path, num_t2_channels=6):

    patients = os.listdir(data_path)
    t1_list = []
    t2_list = []

    for patient in patients:
        patient_path = os.path.join(data_path, patient, 'dwi')
        t1_file = os.path.join(patient_path, 't1_preprocessed.npy')
        t2_file = os.path.join(patient_path, 't2_preprocessed.npy')

        if os.path.exists(t1_file) and os.path.exists(t2_file):
            t1_data = np.load(t1_file)
            t2_data = np.load(t2_file)

            t1_data = np.transpose(t1_data, (3, 0, 1, 2)) #from (1, 512, 512, 10) to (10, 1, 512, 512)
            t1_tensor = torch.from_numpy(t1_data).float()
            #no need to normalize
            t1_tensor = t1_tensor.permute(1, 0, 2, 3)  # (1, 10, 512, 512)

            # Normalizing with the first TE, adding a small epsilon to avoid division by zero
            epsilon = 1e-8  # Small constant to prevent division by zero
            t1_tensor = t1_tensor / torch.clamp(t1_tensor[:, 0:1,:,:], min=epsilon)  # Normalize with first TR, ensuring no zero division

            t2_tensor = t2_tensor[:,1:,:,:] #remove first TR (1, 9, 512, 512)
            t1_list.append(t1_tensor)

            # Determine the indices for the central channels in T2
            num_channels = t2_data.shape[0]
            center_start = (num_channels - num_t2_channels) // 2
            center_end = center_start + num_t2_channels
            selected_indices = list(range(center_start, center_end))

            # Select the central channels from T2
            t2_data_selected = t2_data[selected_indices, :, :, :]  # (6, 512, 512, 9)
            t2_tensor = torch.from_numpy(t2_data_selected).float()
            t2_tensor = t2_tensor.permute(0, 3, 1, 2)  # (6, 9, 512, 512)

            # Normalizing with the first TE, adding a small epsilon to avoid division by zero
            epsilon = 1e-8  # Small constant to prevent division by zero
            t2_tensor = t2_tensor / torch.clamp(t2_tensor[:, 0:1,:,:], min=epsilon)  # Normalize with first TE, ensuring no zero division

            t2_tensor = t2_tensor[:,1:,:,:] #remove first TE (6, 8, 512, 512)
            t2_list.append(t2_tensor)

    return t1_list, t2_list

# Group the preprocessed data
t1_healthy, t2_healthy = group_preprocessed_data(health_path)
t1_osteoporotic, t2_osteoporotic = group_preprocessed_data(osteoporosis_path)
data_size = t1_healthy[0].shape[-1]
t1_slices = t1_healthy[0].shape[0]
num_t1 = t1_healthy[0].shape[1]
t2_slices = t2_healthy[0].shape[0]
num_t2 = t2_healthy[0].shape[1]

# Print the parameters
print(f"data_size: {data_size}")
print(f"t1_slices: {t1_slices}")
print(f"num_t1: {num_t1}")
print(f"t2_slices: {t2_slices}")
print(f"num_t2: {num_t2}")

# Verifica della forma di ogni tensor nel dataset raggruppato
for i, tensor in enumerate(t1_healthy):
    print(f"Shape of grouped_healthy[{i}]: {tensor.shape}")
    break

for i, tensor in enumerate(t2_osteoporotic):
    print(f"Shape of grouped_osteoporotic[{i}]: {tensor.shape}")
    break

# Se desideri ottenere la lunghezza del dataset
print(f"Total number of tensors in grouped_healthy: {len(t1_healthy)}")
print(f"Total number of tensors in grouped_osteoporotic: {len(t2_osteoporotic)}")

class PatientMRI_Dataset(Dataset):
  def __init__(self, t1_data, t2_data, label):
      self.t1_images = t1_data
      self.t2_images = t2_data
      # Assign the label to each image, assuming each patient has a single label
      self.labels = [label] * len(t1_data)  # All images have the same

  def __len__(self):
    return len(self.t1_images)

  def __getitem__(self, idx):
    t1_image = self.t1_images[idx]
    t2_image = self.t2_images[idx]
    # Convert label to one-hot encoding
    label = torch.tensor(self.labels[idx], dtype=torch.long).unsqueeze(0)  # Scalar integer label
    #label = torch.nn.functional.one_hot(label, num_classes=2).float()  # One-hot encoding
    return t1_image, t2_image, label  # Return the images and its corresponding label

# Creare il dataset combinato
combined_dataset = ConcatDataset([PatientMRI_Dataset(t1_healthy, t2_healthy, label=0), PatientMRI_Dataset(t1_osteoporotic, t2_osteoporotic, label=1)])

"""### Classificatore"""

class MRI_Classifier(nn.Module):
    def __init__(self, kernel_size=3, stride=2, padding=1, data_size=512, t1_slices=1, t2_slices=6, num_t2=8, num_t1=10):
        super(MRI_Classifier, self).__init__()

        self.data_size = data_size
        self.num_t1 = num_t1
        self.num_t2 = num_t2
        self.t1_slices = t1_slices
        self.t2_slices = t2_slices
        self.hidden_size = 64

        # Convolutional layer for linear fit across 9 versions (t2)
        self.fit_conv_t2 = nn.Conv3d(in_channels=t2_slices, out_channels=t2_slices, kernel_size=(num_t2, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))

        # Additional convolutional layers for t2 with groups=6, each convolution will halved the data size
        self.conv1_t2 = nn.Conv2d(in_channels=t2_slices, out_channels=t2_slices*12, kernel_size=kernel_size, stride=stride, padding=padding, groups=t2_slices)
        self.conv2_t2 = nn.Conv2d(in_channels=t2_slices*12, out_channels=t2_slices*24, kernel_size=kernel_size, stride=stride, padding=padding, groups=t2_slices)
        self.conv3_t2 = nn.Conv2d(in_channels=t2_slices*24, out_channels=t2_slices*36, kernel_size=kernel_size, stride=stride, padding=padding, groups=t2_slices)

        # Max pooling layer
        self.maxpool_t2 = nn.MaxPool2d(kernel_size=2,stride=2)
        # Max pooling layer across the 6 images' channels
        self.avgpool_t2 = nn.AvgPool3d(kernel_size=(36, 1,1), stride=(36,1,1))  # Pool across the "6 images" dimension

        # Convolutional layer for linear fit across 10 versions (t1)
        self.fit_conv_t1 = nn.Conv3d(in_channels=t1_slices, out_channels=t1_slices, kernel_size=(num_t1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))

        # Additional convolutional layers for t1
        self.conv1_t1 = nn.Conv2d(in_channels=t1_slices, out_channels=10*t1_slices, kernel_size=kernel_size, stride=stride, padding=padding, groups=t1_slices)
        self.conv2_t1 = nn.Conv2d(in_channels=10*t1_slices, out_channels=20*t1_slices, kernel_size=kernel_size, stride=stride, padding=padding,groups=t1_slices)
        self.conv3_t1 = nn.Conv2d(in_channels=20*t1_slices, out_channels=30*t1_slices, kernel_size=kernel_size, stride=stride, padding=padding,groups=t1_slices)

        # Max pooling layer
        self.maxpool_t1 = nn.MaxPool2d(kernel_size=2,stride=2)
        # Max pooling layer across the 6 images' channels
        self.avgpool_t1 = nn.AvgPool3d(kernel_size=(30, 1,1), stride=(30,1,1))  # Pool across the "6 images" dimension

        # Flatten Images
        self.flatten = nn.Flatten()

        # Classifier
        # Input features to Linear layer: height * width * 2) after concatenation
        self.fc = nn.Sequential(
            nn.Linear((data_size//8) * (data_size//8) * (t1_slices+t2_slices), self.hidden_size),
            nn.ReLU(),  # Activation function
            nn.Linear(self.hidden_size, 1)#, one output for binay calssification
            #nn.Sigmoid()  # Sigmoid for binary classification,a lready in Loss
        )
        self.relu = nn.ReLU()

    def forward(self, t1, t2):
        # Apply logarithmic transformation to the input data
        epsilon = 1e-8  # Small constant to avoid log(0)

        # Process t2
        x = torch.log(t2 + epsilon)
        x = self.fit_conv_t2(x)  # Output shape: (batch_size, 6, 1, height, width)
        x = x.squeeze(2)  # Remove the version dimension: (batch_size, 6, height, width)

        # Apply additional convolutional layers to process independently
        x = self.relu(self.conv1_t2(x))  # Output shape: (batch_size, 6*12, height//2, width//2)
        #x = self.maxpool_t2(x)  # Output shape: (batch_size, 6*12, height//4, width//4)
        x = self.relu(self.conv2_t2(x))  # Output shape: (batch_size, 6*24, height//8, width//8)
        #x = self.maxpool_t2(x)  # Output shape: (batch_size, 6*24, height//16, width//16)
        x = self.conv3_t2(x)  # Output shape: (batch_size, 6*36, height//32, width//32)

        # Apply avg pooling across the 6 image channels
        x = self.avgpool_t2(x)  # Output shape: (batch_size, 6, height//32, width//32)

        # Process t1, log non è top ma per ora lo teniamo
        y = torch.log(t1 + epsilon)
        y = self.fit_conv_t1(y)  # Output shape: (batch_size, 1, 1, height, width)
        y = y.squeeze(2)  # Remove the version dimension: (batch_size, 1, height, width)

        # Apply additional convolutional layers
        y = self.relu(self.conv1_t1(y))  # Output shape: (batch_size, 10, height//2, width//2)
        #y = self.maxpool_t1(y)  # Output shape: (batch_size, 10, height//4, width//4)
        y = self.relu(self.conv2_t1(y))  # Output shape: (batch_size, 20, height//8, width//8)
        #y = self.maxpool_t1(y)  # Output shape: (batch_size, 20, height//16, width//16)
        y = self.conv3_t1(y)  # Output shape: (batch_size, 30, height//32, width//32)

        # Avg pooling layer across the  image's channels
        y = self.avgpool_t1(y)  # Output shape: (batch_size, 1, height//32, width//32)

        # Concatenate the processed outputs
        z = torch.cat((x, y), dim=1)  # Concatenate along the channel dimension: (batch_size, 1+6, height//32, width//32)

        latent = self.flatten(z)  # Flatten the tensor for classification

        pos_class = self.fc(latent)  # Final classification

        return latent, pos_class

"""### Training"""

# Stampo il modello

model = MRI_Classifier(data_size=data_size, t1_slices=t1_slices, t2_slices=t2_slices, num_t2=num_t2, num_t1=num_t1).to(device)
summary(model, input_size=[(1, 10, 512, 512), (6, 8, 512, 512)])

# Function to evaluate the model
def evaluate_model(model, dataloader, device):
    model.eval()
    all_labels = []
    all_preds = []
    all_probs = []

    with torch.no_grad():
        for t1, t2, labels in dataloader:
            t1 = t1.to(device)
            t2 = t2.to(device)
            labels = labels.float().cpu().numpy()

            _, outputs = model(t1, t2)
            # Apply sigmoid to get probabilities
            probs = torch.sigmoid(outputs).cpu().numpy().flatten()
            preds = (probs > 0.5).astype(int)  # Convert probabilities to binary predictions

            all_labels.extend(labels)
            all_preds.extend(preds)
            all_probs.extend(probs)

    # Convert lists to numpy arrays
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    all_probs = np.array(all_probs)

    return all_labels, all_preds, all_probs

# Fissare il seme per garantire risultati riproducibili
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(40)

# Define pos_weight as a tensor with the computed weight
pos_weight = len(t1_healthy) / len(t1_osteoporotic)
pos_weight = torch.tensor(pos_weight, dtype=torch.float32)
print("pos_weight =", pos_weight.item())

# Hyperparameters
lr = 0.0015
weight_decay = 1e-5
num_epochs = 30

if False:
  n = len(combined_dataset)
  accuracies = []
  precisions = []
  recalls = []
  f1_scores = []
  conf_matrices = []
  jack_losses = []
  total_labels = []
  total_preds = []
  total_probs = []

  for i in range(n):
      print(f"Jackknife iteration {i+1}/{n}")

      # Create train and validation sets
      validation_set = torch.utils.data.Subset(combined_dataset, [i])
      train_set = torch.utils.data.Subset(combined_dataset, list(range(i)) + list(range(i+1, n)))

      train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=2)
      validation_loader = DataLoader(validation_set, batch_size=1, shuffle=False)

      # Initialize the model
      model = MRI_Classifier(data_size=data_size, t1_slices=t1_slices, t2_slices=t2_slices, num_t2=num_t2, num_t1=num_t1)
      model = model.to(device)
      criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

      # Training loop
      epochs_losses = []
      for epoch in range(num_epochs):
          model.train()
          running_loss = 0.0

          for t1, t2, labels in train_loader:
              t1 = t1.to(device)
              t2 = t2.to(device)
              labels = labels.float().to(device)

              optimizer.zero_grad()
              _, outputs = model(t1, t2)
              loss = criterion(outputs, labels)
              loss.backward()
              optimizer.step()

              running_loss += loss.item()

          epochs_losses.append(running_loss / len(train_loader))

      jack_losses.append(epochs_losses)
      torch.cuda.empty_cache()

      # Evaluate on the validation set
      labels, preds, probs = evaluate_model(model, validation_loader, device)

      total_labels.extend(labels)
      total_preds.extend(preds)
      total_probs.extend(probs)  # Probability for the positive class

  # Convert lists to numpy arrays
  total_labels = np.array(total_labels).flatten()
  total_preds = np.array(total_preds)
  total_probs = np.array(total_probs)
  print(f"Shape of total_labels: {total_labels.shape}")
  print(f"Shape of total_preds: {total_preds.shape}")
  print(f"Shape of total_probs: {total_probs.shape}")

  # Calculate global metrics
  accuracy = accuracy_score(total_labels, total_preds)
  precision, recall, f1_score, _ = precision_recall_fscore_support(total_labels, total_preds, average='binary', zero_division=0)
  conf_matrix = confusion_matrix(total_labels, total_preds, labels=[0, 1],normalize='all')

  # Calculate the ROC curve
  fpr, tpr, _ = roc_curve(total_labels, total_probs)
  roc_auc = auc(fpr, tpr)

  # Print summary
  print(f"Jackknife Cross-Validation Results:")
  print(f"Accuracy: {accuracy:.4f}")
  print(f"Precision: {precision:.4f}")
  print(f"Recall: {recall:.4f}")
  print(f"F1-Score: {f1_score:.4f}")
  print(f"Confusion Matrix:\n{conf_matrix}")

  # Plot the ROC curve
  plt.figure()
  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver Operating Characteristic')
  plt.legend(loc="lower right")
  plt.show()

  # Average loss over epochs
  avg_loss = np.mean(jack_losses, axis=0)

  # Plot the average loss over epochs
  plt.plot(range(num_epochs+1),avg_loss, label='Average Training Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.title('Training Loss Over Epochs')
  plt.legend()
  plt.show()

else:
  print("No jackknive")

"""### Riassunto

Abbiamo sviluppato e testato il classificatore per distinguere tra pazienti osteoporotici e sani, utilizzando una rete neurale convoluzionale che processa separatamente le acquisizioni temporali delle immagini MRI $T_1$ e $T_2$, e le combina per effettuare la classificazione finale.

1. **Progettazione del Modello**:
   - È stato progettato un modello chiamato `MRI_Classifier`, che utilizza convoluzioni 3D per adattare i canali $T_1$ e $T_2$, seguite da più strati convoluzionali 2D con gruppi per elaborare separatamente le informazioni. Dopo aver estratto le caratteristiche, queste sono state combinate e appiattite per la classificazione finale.

2. **Addestramento del Modello**:
   - Il modello è stato addestrato utilizzando una Jackknife Cross-Validation su 20 epoche. Abbiamo utilizzato la funzione di perdita `BCEWithLogitsLoss` con un bilanciamento del peso positivo per gestire lo sbilanciamento delle classi (21 pazienti **o**steoporotici contro 12 **h**ealthy), e l'ottimizzatore Adam per l'aggiornamento dei pesi.

3. **Jackknife Cross-Validation**:
   - Ogni campione è stato escluso a turno dal training set e utilizzato per la validazione, fornendo una stima delle prestazioni del modello. Questo ha permesso di valutare il modello su tutti i dati disponibili, migliorando l'affidabilità delle metriche di valutazione.

4. **Risultati della Valutazione**:
   - Le prestazioni del modello sono state moderate:
    - **Accuracy**: 55%
     - **Precision**: 62%
     - **Recall**: 76%
     - **F1-Score**: 68%

 L'*accuratezza* generale è bassa, indicando che il modello fatica a classificare correttamente i campioni nel loro insieme.
 La *precisione* suggerisce che oltre la metà delle previsioni positive sono corrette, ma c'è ancora una significativa proporzione di falsi positivi.
 Il *richiamo* è relativamente alto, indicando che il modello identifica la maggior parte dei campioni positivi, ma potrebbe farlo a scapito di un maggior numero di falsi positivi.
 L'*F1-Score*, un compromesso tra precisione e richiamo, mostra che il modello ha prestazioni moderate, ma c'è spazio per migliorare il bilanciamento tra falsi positivi e falsi negativi.
     
      La matrice di confusione - inoltre - ha mostrato che il modello ha difficoltà a distinguere correttamente tra le due classi, specialmente nella classe sana:

\begin{bmatrix}
0.06 & 0.30 \\
0.15 & 0.49 \\
\end{bmatrix}


5. **Valutazione delle Prestazioni**:
   - Abbiamo analizzato le prestazioni del modello utilizzando metriche come accuracy, precision, recall, e F1-score. I risultati indicano che, sebbene il modello sia in grado di identificare alcune caratteristiche utili, la sua capacità discriminativa è limitata, suggerendo la necessità di ulteriori miglioramenti.


6. **ROC curve & AUC**
   Otteniamo: AUC = 0.36 < 0.50	 $\rightarrow$  il modello è peggiore di un classificatore casuale. Questo potrebbe indicare che esso inverte le classi, assegnando probabilità alte alla classe sbagliata.

**Il forte *imbalance* delle classi ne è sicuramente una causa.**


### Conclusione

Il classificatore sviluppato ha mostrato una performance moderata nel distinguere tra pazienti osteoporotici e sani, con metriche che indicano un margine significativo per miglioramenti. La Jackknife Cross-Validation ha fornito una stima affidabile delle prestazioni, ma i risultati suggeriscono che il modello potrebbe beneficiare di ulteriori ottimizzazioni e di un bilanciamento migliore delle classi.

##3*a*) Feature Selection: *ICA*

###**Cosa è la Independent Component Analysis (ICA)?**

E' una tecnica di decomposizione statistica che viene utilizzata per separare un set di segnali misti in componenti statisticamente indipendenti. È un metodo particolarmente utile quando si lavora con dati che sono generati dalla combinazione di sorgenti indipendenti ma non osservabili direttamente.

L'obiettivo principale dell'ICA è trovare una rappresentazione in cui le componenti (o "sorgenti") siano il più possibile indipendenti tra loro. Questo è utile in molte applicazioni, come la separazione delle sorgenti sonore, l'elaborazione di segnali biomedicali (ad esempio, EEG) ed anche la riduzione della dimensionalità in machine learning.

###**Come Funziona?**
L'ICA si basa sull'assunzione che le sorgenti siano statisticamente indipendenti e che il mixing sia lineare. La tecnica cerca di trovare una matrice di pesi che, quando applicata ai dati osservati, produce componenti indipendenti.

A livello pratico, l'ICA può essere vista come un'estensione della Principal Component Analysis (PCA), ma mentre la PCA massimizza la varianza e trova componenti ortogonali, l'ICA si concentra sulla massimizzazione dell'indipendenza statistica delle componenti.
"""

# Checkpoints (to save model parameters during training)
class SaveBestModel:
    def __init__(self, best_loss=float('inf')):
        self.best_loss = best_loss

    def __call__(self, current_loss, model, optimizer, criterion, epoch, model_name='MRI_Classifier'):
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            print(f"\nBest validation loss: {self.best_loss}")
            print(f"Saving best model for epoch: {epoch+1}\n")
            torch.save({
                'epoch': epoch+1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': criterion,
            }, f'/content/drive/MyDrive/osteoporosi/best_{model_name}.pth')

# Fissare il seme per garantire risultati riproducibili
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(40)

def extract_latent_features(model, dataset, device):
    data_loader = DataLoader(dataset, batch_size=4, shuffle=False)
    model.eval()
    all_latent_features = []
    all_labels = []

    with torch.no_grad():
        for t1, t2, labels in data_loader:
            t1 = t1.to(device)
            t2 = t2.to(device)

            latent, _ = model(t1, t2)
            labels = labels.float()  # One-hot encoding

            all_latent_features.append(latent.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    latent_features = np.concatenate(all_latent_features, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)  # Concatena correttamente le etichette
    print(f"Shape delle caratteristiche latenti: {latent_features.shape}")
    print(f"Shape delle etichette: {all_labels.shape}")
    return latent_features, all_labels

def perform_ica(latent_features, all_labels, n_components=2):

    # Perform ICA
    ica = FastICA(n_components=n_components, random_state=42)
    latent_ica = ica.fit_transform(latent_features)
    cmap = mcolors.ListedColormap(['g', 'r'])
    bounds = [0, 0.5, 1]
    norm = mcolors.BoundaryNorm(bounds, cmap.N)


    plt.figure(figsize=(7, 5))
    scatter = plt.scatter(latent_ica[:, 0], latent_ica[:, 1], c=all_labels, marker='o', alpha=0.7, cmap=cmap)
    cbar = plt.colorbar(scatter, ticks=[0, 1])
    plt.title('ICA delle Caratteristiche Latenti')
    plt.xlabel('Componente ICA 1')
    plt.ylabel('Componente ICA 2')
    plt.grid(False)
    plt.show()

    return latent_ica

# Creazione del DataLoader per il training su tutti i dati
data_loader = DataLoader(combined_dataset, batch_size=11, shuffle=True)

# Define pos_weight as a tensor with the computed weight
pos_weight = len(t1_healthy) / len(t1_osteoporotic)
pos_weight = torch.tensor(pos_weight, dtype=torch.float32)
print("pos_weight =", pos_weight.item())

# Hyperparameters
lr = 0.0015
weight_decay = 1e-5
num_epochs = 30

# Initialize the model
model = MRI_Classifier(data_size=data_size, t1_slices=t1_slices, t2_slices=t2_slices, num_t2=num_t2, num_t1=num_t1)
model = model.to(device)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

# List to store training loss for each epoch
train_losses = []
save_best_model = SaveBestModel()

# Ciclo di training
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for t1, t2, labels in data_loader:
        t1 = t1.to(device)
        t2 = t2.to(device)
        labels = labels.float().to(device)

        optimizer.zero_grad()
        _, outputs = model(t1, t2)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    running_loss /= len(data_loader)
    save_best_model(running_loss, model, optimizer, criterion, epoch)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}")

    # Append the running loss to the list
    train_losses.append(running_loss)

plt.figure(figsize=(7, 5))
plt.plot(range(1, num_epochs+1), train_losses, marker='o', color='g', label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.grid(False)
plt.show()

# Questo disabilita tutti gli avvisi
warnings.filterwarnings("ignore")

# Carica il modello migliore
best_model = torch.load('/content/drive/MyDrive/osteoporosi/best_MRI_Classifier.pth')
model.load_state_dict(best_model['model_state_dict'])


# Estrazione delle caratteristiche latenti
latent_features, all_labels = extract_latent_features(model, combined_dataset, device)

# Visualizzazione delle caratteristiche latenti
print("\n")

# Implementazione dell'ICA e visualizzazione dei risultati
latent_ica = perform_ica(latent_features, all_labels, n_components=2)

"""### Riassunto

Abbiamo progettato un workflow completo per addestrare un modello di classificazione MRI, salvare il miglior modello durante il training, e successivamente visualizzare le caratteristiche latenti tramite ICA (Independent Component Analysis).

1. **Impostazione del Training e Salvataggio del Miglior Modello**:
   - Addestriamo il modello `MRI_Classifier` utilizzando un ciclo di training su 30 epoche, monitorando la `loss` di addestramento.
   Durante il training, abbiamo utilizzato la classe `SaveBestModel` per salvare il modello con la miglior loss di validazione su Google Drive.
   - La `loss` è stata calcolata utilizzando la funzione di perdita `BCEWithLogitsLoss`, che integra una ponderazione per gestire lo sbilanciamento delle classi, e il modello è stato ottimizzato con l'ottimizzatore `Adam`.

2. **Estrazione delle Caratteristiche Latenti**:
   - Una volta completato l'addestramento, abbiamo caricato il miglior modello salvato e lo abbiamo utilizzato per estrarre le caratteristiche latenti dalle immagini MRI. Questo è stato fatto utilizzando la funzione `extract_latent_features`, che raccoglie le caratteristiche latenti generate dal modello.

4. **Implementazione dell'ICA e Visualizzazione**:
   - Abbiamo applicato l'Independent Component Analysis (ICA) alle caratteristiche latenti estratte per ridurre la dimensionalità a 2 componenti. L'ICA ci permette di visualizzare le caratteristiche latenti in uno spazio bidimensionale, evidenziando le relazioni indipendenti tra le variabili latenti.

### Osservazioni sul Grafico ICA ottenuto:

* **Separabilità delle Classi**:
   - Nel grafico, vediamo una sovrapposizione significativa tra i punti rossi (pazienti osteoporotici) e i punti verdi (pazienti sani). Ciò suggerisce che le componenti indipendenti trovate tramite ICA non separano chiaramente le due classi.
   - Questo indica che le caratteristiche latenti, dopo la trasformazione ICA, non riescono a distinguere chiaramente tra i pazienti sani e quelli osteoporotici.

* **Interpretazione delle Componenti**:
   - Le componenti ICA sono lineari combinazioni delle caratteristiche originali e sono progettate per massimizzare l'indipendenza statistica tra di esse. Tuttavia, l'obiettivo dell'ICA non è necessariamente quello di massimizzare la separazione delle classi, ma piuttosto di trovare rappresentazioni statisticamente indipendenti.

4. **Implicazioni**:
   - La sovrapposizione delle due classi potrebbe suggerire che la ristrettezza del dataset può influire significativamente sui risultati finali, oppure la necessità di esplorare altre tecniche di riduzione della dimensionalità.

##3*b*) Feature Selection *II:* *PCA*
Stesso procedimento di *ICA*, ma vediamo se con la *Principal Component Analysis* osserviamo qualcosa di diverso...
"""

def perform_pca(latent_features, labels, n_components=2):
    # Creare il modello PCA
    pca = PCA(n_components=n_components)
    latent_pca = pca.fit_transform(latent_features)

    # Definire la mappa dei colori per le classi
    cmap = mcolors.ListedColormap(['g', 'r'])
    bounds = [0, 0.5, 1]
    norm = mcolors.BoundaryNorm(bounds, cmap.N)


    # Visualizzare i risultati della PCA
    plt.figure(figsize=(7, 5))
    scatter = plt.scatter(latent_pca[:, 0], latent_pca[:, 1], c=labels, marker='o', alpha=0.7, cmap=cmap)
    cbar = plt.colorbar(scatter, ticks=[0, 1])
    plt.title('PCA delle Caratteristiche Latenti')
    plt.xlabel('Componente PCA 1')
    plt.ylabel('Componente PCA 2')
    plt.grid(False)
    plt.show()

    return latent_pca

# Esegui la PCA sulle caratteristiche latenti
latent_pca = perform_pca(latent_features, all_labels, n_components=2)

"""La PCA, a differenza di ICA, si concentra nel preservare e massimizzare la varianza dei dati nella rappresentazione a bassa dimensionalità. Tuttavia entrambi sono approcci lineari, che faticano a individuare strutture più complesse nei dati. Altri metodi di visualizzazione di spazi latenti sono t-SNE e UMAP, che sono non lineari. Proviamo a verificare se sono in grando di catturare strutture di distribuzione dei dati."""

# Apply t-SNE to reduce the latent space to 2D
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
latent_2d = tsne.fit_transform(latent_features)

# Define the color map and normalization
cmap = mcolors.ListedColormap(['g', 'r'])
bounds = [0, 0.5, 1]
norm = mcolors.BoundaryNorm(bounds, cmap.N)

# Visualize the 2D t-SNE result
plt.figure(figsize=(8, 6))
scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=all_labels, cmap=cmap, s=5)
plt.colorbar(scatter,ticks=[0,1])
plt.title('t-SNE Visualization of Latent Space')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()

# Apply UMAP to reduce the latent space to 2D
umap_reducer = umap.UMAP(n_neighbors=5, min_dist=0.2, n_components=2, random_state=42)
latent_2d = umap_reducer.fit_transform(latent_features)

# Visualize the 2D UMAP result
plt.figure(figsize=(8, 6))
scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=all_labels, cmap=cmap, s=5)
plt.colorbar(scatter,ticks=[0,1])
plt.title('UMAP Visualization of Latent Space')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.show()

"""Anche cambiano i parametri di t-SNE (*perplexity*) e di UMAP (*n_neighbours*, *min_dist*) non si ottiene una visualizazione delle classi ben separate. Il modello non riesce a trovare uno spazio latente in cui separare bene le classi, come ci aspettvamo dai risultati (*accuracy*, *conf_matrix*, *roc_curve*, ecc) del Jackknife, e da cui individuare due componenti principali per la visualizzazione con i metodi appena utilizzati."""